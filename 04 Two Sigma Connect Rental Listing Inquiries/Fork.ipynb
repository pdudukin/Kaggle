{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from itertools import product\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "import functools as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    fmt = lambda s: s.replace(\"\\u00a0\", \"\").strip().lower()\n",
    "    df[\"photo_count\"] = df[\"photos\"].apply(len)\n",
    "    df[\"street_address\"] = df['street_address'].apply(fmt)\n",
    "    df[\"display_address\"] = df[\"display_address\"].apply(fmt)\n",
    "    df[\"desc_wordcount\"] = df[\"description\"].apply(len)\n",
    "    df[\"pricePerBed\"] = df['price'] / df['bedrooms']\n",
    "    df[\"pricePerBath\"] = df['price'] / df['bathrooms']\n",
    "    df[\"pricePerRoom\"] = df['price'] / (df['bedrooms'] + df['bathrooms'])\n",
    "    df[\"bedPerBath\"] = df['bedrooms'] / df['bathrooms']\n",
    "    df[\"bedBathDiff\"] = df['bedrooms'] - df['bathrooms']\n",
    "    df[\"bedBathSum\"] = df[\"bedrooms\"] + df['bathrooms']\n",
    "    df[\"bedsPerc\"] = df[\"bedrooms\"] / (df['bedrooms'] + df['bathrooms'])\n",
    "\n",
    "    df = df.fillna(-1).replace(np.inf, -1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def factorize(df1, df2, column):\n",
    "    ps = df1[column].append(df2[column])\n",
    "    factors = ps.factorize()[0]\n",
    "    df1[column] = factors[:len(df1)]\n",
    "    df2[column] = factors[len(df1):]\n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "def designate_single_observations(df1, df2, column):\n",
    "    ps = df1[column].append(df2[column])\n",
    "    grouped = ps.groupby(ps).size().to_frame().rename(columns={0: \"size\"})\n",
    "    df1.loc[df1.join(grouped, on=column, how=\"left\")[\"size\"] <= 1, column] = -1\n",
    "    df2.loc[df2.join(grouped, on=column, how=\"left\")[\"size\"] <= 1, column] = -1\n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "def hcc_encode(train_df, test_df, variable, target, prior_prob, k, f=1, g=1, r_k=None, update_df=None):\n",
    "    \"\"\"\n",
    "    See \"A Preprocessing Scheme for High-Cardinality Categorical Attributes in\n",
    "    Classification and Prediction Problems\" by Daniele Micci-Barreca\n",
    "    \"\"\"\n",
    "    hcc_name = \"_\".join([\"hcc\", variable, target])\n",
    "\n",
    "    grouped = train_df.groupby(variable)[target].agg({\"size\": \"size\", \"mean\": \"mean\"})\n",
    "    grouped[\"lambda\"] = 1 / (g + np.exp((k - grouped[\"size\"]) / f))\n",
    "    grouped[hcc_name] = grouped[\"lambda\"] * grouped[\"mean\"] + (1 - grouped[\"lambda\"]) * prior_prob\n",
    "\n",
    "    df = test_df[[variable]].join(grouped, on=variable, how=\"left\")[hcc_name].fillna(prior_prob)\n",
    "    if r_k: df *= np.random.uniform(1 - r_k, 1 + r_k, len(test_df))     # Add uniform noise. Not mentioned in original paper\n",
    "\n",
    "    if update_df is None: update_df = test_df\n",
    "    if hcc_name not in update_df.columns: update_df[hcc_name] = np.nan\n",
    "    update_df.update(df)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train = pd.read_json(\"train.json\").sort_values(by=\"listing_id\")\n",
    "X_test = pd.read_json(\"test.json\").sort_values(by=\"listing_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make target integer, one hot encoded, calculate target priors\n",
    "X_train = X_train.replace({\"interest_level\": {\"low\": 0, \"medium\": 1, \"high\": 2}})\n",
    "X_train = X_train.join(pd.get_dummies(X_train[\"interest_level\"], prefix=\"pred\").astype(int))\n",
    "prior_0, prior_1, prior_2 = X_train[[\"pred_0\", \"pred_1\", \"pred_2\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add common features\n",
    "X_train = add_features(X_train)\n",
    "X_test = add_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Special designation for building_ids, manager_ids, display_address with only 1 observation\n",
    "for col in ('building_id', 'manager_id', 'display_address'):\n",
    "    X_train, X_test = designate_single_observations(X_train, X_test, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# High-Cardinality Categorical encoding\n",
    "skf = StratifiedKFold(5)\n",
    "attributes = product((\"building_id\", \"manager_id\"), zip((\"pred_1\", \"pred_2\"), (prior_1, prior_2)))\n",
    "for variable, (target, prior) in attributes:\n",
    "    hcc_encode(X_train, X_test, variable, target, prior, k=5, r_k=None)\n",
    "    for train, test in skf.split(np.zeros(len(X_train)), X_train['interest_level']):\n",
    "        hcc_encode(X_train.iloc[train], X_train.iloc[test], variable, target, prior, k=5, r_k=0.01, update_df=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Factorize building_id, display_address, manager_id, street_address\n",
    "for col in ('building_id', 'display_address', 'manager_id', 'street_address'):\n",
    "    X_train, X_test = factorize(X_train, X_test, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create binarized features\n",
    "fmt = lambda feat: [s.replace(\"\\u00a0\", \"\").strip().lower().replace(\" \", \"_\") for s in feat]  # format features\n",
    "X_train[\"features\"] = X_train[\"features\"].apply(fmt)\n",
    "X_test[\"features\"] = X_test[\"features\"].apply(fmt)\n",
    "features = [f for f_list in list(X_train[\"features\"]) + list(X_test[\"features\"]) for f in f_list]\n",
    "ps = pd.Series(features)\n",
    "grouped = ps.groupby(ps).agg(len)\n",
    "features = grouped[grouped >= 10].index.sort_values().values    # limit to features with >=10 observations\n",
    "mlb = MultiLabelBinarizer().fit([features])\n",
    "columns = ['feature_' + s for s in mlb.classes_]\n",
    "flt = lambda l: [i for i in l if i in mlb.classes_]     # filter out features not present in MultiLabelBinarizer\n",
    "X_train = X_train.join(pd.DataFrame(data=mlb.transform(X_train[\"features\"].apply(flt)), columns=columns, index=X_train.index))\n",
    "X_test = X_test.join(pd.DataFrame(data=mlb.transform(X_test[\"features\"].apply(flt)), columns=columns, index=X_test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "\n",
    "X_train = X_train.sort_index(axis=1).sort_values(by=\"listing_id\")\n",
    "X_test = X_test.sort_index(axis=1).sort_values(by=\"listing_id\")\n",
    "columns_to_drop = [\"photos\", \"pred_0\",\"pred_1\", \"pred_2\", \"description\", \"features\", \"created\"]\n",
    "X_train.drop([c for c in X_train.columns if c in columns_to_drop], axis=1).\\\n",
    "    to_csv(\"train_python.csv\", index=False, encoding='utf-8')\n",
    "X_test.drop([c for c in X_test.columns if c in columns_to_drop], axis=1).\\\n",
    "    to_csv(\"test_python.csv\", index=False, encoding='utf-8')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read new data\n",
    "train = pd.read_csv(\"train_python.csv\")\n",
    "test = pd.read_csv(\"test_python.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 286)\n",
      "(74659, 285)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = train.columns\n",
    "cols = cols.drop(['listing_id','interest_level'])\n",
    "train_X = train[cols]\n",
    "train_y = train['interest_level']\n",
    "test_X = test[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rft = RandomForestClassifier(n_jobs=-1)\n",
    "params = {\n",
    "    'n_estimators' : [50,100,200,400],\n",
    "    'max_features' : ['auto'],\n",
    "    'max_depth' : [3,5,10,20,50],\n",
    "    'min_samples_leaf' : [0.003],\n",
    "    'min_samples_split' : [0.001],\n",
    "    'criterion' : ['entropy','gini'],\n",
    "    'class_weight' : ['balanced',None],\n",
    "    'bootstrap' : [True],\n",
    "    'oob_score' : [False],\n",
    "    'random_state' : [0,123,12345]\n",
    "}\n",
    "def framework(clf,params,n_iter):\n",
    "    # calculate # of iterations for Search\n",
    "    parsize = ft.reduce(lambda a,b: a*b,[len(params[x]) for x in params]) # total # of combinations\n",
    "    psize = n_iter if parsize > n_iter else parsize # limit # by n_iter\n",
    "    print ('Parameters combination :',str(psize)+\"/\"+str(parsize))   \n",
    "    \n",
    "    rgs = RandomizedSearchCV(\n",
    "    estimator = clf,\n",
    "    param_distributions = params,\n",
    "    n_iter = psize,\n",
    "    scoring = 'neg_log_loss',\n",
    "    n_jobs = -1,\n",
    "    cv = 5,\n",
    "    refit=True,\n",
    "    verbose=1)\n",
    "\n",
    "    rgs.fit(train_X,train_y)\n",
    "    \n",
    "    n_top = 3\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(rgs.cv_results_['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i), \"(best model)\" if rgs.best_index_ == candidate else \"\")\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  rgs.cv_results_['mean_test_score'][candidate],\n",
    "                  rgs.cv_results_['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(rgs.cv_results_['params'][candidate]))\n",
    "    \n",
    "    return rgs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters combination : 50/1800\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 82.4min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 279.2min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 353.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1 (best model)\n",
      "Mean validation score: -0.579 (std: 0.044)\n",
      "Parameters: {'subsample': 0.75, 'objective': 'multi:softprob', 'nthread': -1, 'n_estimators': 75, 'min_child_weight': 0.01, 'max_depth': 7, 'learning_rate': 0.15, 'gamma': 0.1}\n",
      "Model with rank: 2 \n",
      "Mean validation score: -0.579 (std: 0.054)\n",
      "Parameters: {'subsample': 0.75, 'objective': 'multi:softprob', 'nthread': -1, 'n_estimators': 200, 'min_child_weight': 0.005, 'max_depth': 5, 'learning_rate': 0.15, 'gamma': 0.9}\n",
      "Model with rank: 3 \n",
      "Mean validation score: -0.580 (std: 0.034)\n",
      "Parameters: {'subsample': 0.75, 'objective': 'multi:softprob', 'nthread': -1, 'n_estimators': 100, 'min_child_weight': 0.01, 'max_depth': 5, 'learning_rate': 0.1, 'gamma': 0.8}\n"
     ]
    }
   ],
   "source": [
    "xgb = xgb.XGBClassifier(nthread=-1)\n",
    "#print(xgb.get_params().keys())\n",
    "xgb_params = {\"max_depth\": [3,5,7],\n",
    "              \"learning_rate\": [0.01,0.05,0.1,0.15],\n",
    "              \"n_estimators\": [50,75,100,150,200],\n",
    "              \"min_child_weight\": [0.01,0.005],\n",
    "              \"gamma\": [0.1,0.5,0.8,0.9,1.0],\n",
    "              \"subsample\":[0.75,0.9,1.0],\n",
    "              #\"eval_metric\":['logloss'],\n",
    "              \"objective\":[\"multi:softprob\"],\n",
    "              #\"seed\":[0],\n",
    "              \"nthread\":[-1]\n",
    "              #,\"eval_metric\": ['roc_auc']\n",
    "             }\n",
    "\n",
    "est = framework(xgb,xgb_params,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame(est.predict_proba(test_X),columns=est.classes_)\n",
    "result['listing_id'] = test['listing_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>listing_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065015</td>\n",
       "      <td>0.203845</td>\n",
       "      <td>0.731141</td>\n",
       "      <td>6811958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.312899</td>\n",
       "      <td>0.398462</td>\n",
       "      <td>0.288639</td>\n",
       "      <td>6811960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.122461</td>\n",
       "      <td>0.357515</td>\n",
       "      <td>0.520024</td>\n",
       "      <td>6811964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.957467</td>\n",
       "      <td>0.035353</td>\n",
       "      <td>0.007180</td>\n",
       "      <td>6811971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.733189</td>\n",
       "      <td>0.111023</td>\n",
       "      <td>0.155787</td>\n",
       "      <td>6811974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.951445</td>\n",
       "      <td>0.030730</td>\n",
       "      <td>0.017825</td>\n",
       "      <td>6811983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.410698</td>\n",
       "      <td>0.431817</td>\n",
       "      <td>0.157485</td>\n",
       "      <td>6811984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.933338</td>\n",
       "      <td>0.030411</td>\n",
       "      <td>0.036251</td>\n",
       "      <td>6811985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.661238</td>\n",
       "      <td>0.146402</td>\n",
       "      <td>0.192360</td>\n",
       "      <td>6811988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.984740</td>\n",
       "      <td>0.011803</td>\n",
       "      <td>0.003457</td>\n",
       "      <td>6811990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.855623</td>\n",
       "      <td>0.087344</td>\n",
       "      <td>0.057032</td>\n",
       "      <td>6811992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.981936</td>\n",
       "      <td>0.012301</td>\n",
       "      <td>0.005763</td>\n",
       "      <td>6811995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.995360</td>\n",
       "      <td>0.004043</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>6811997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.995629</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>6812012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.997648</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>6812016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.933809</td>\n",
       "      <td>0.050804</td>\n",
       "      <td>0.015388</td>\n",
       "      <td>6812032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.974869</td>\n",
       "      <td>0.022679</td>\n",
       "      <td>0.002451</td>\n",
       "      <td>6812035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.979312</td>\n",
       "      <td>0.013545</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>6812041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.953315</td>\n",
       "      <td>0.044381</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>6812045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.973253</td>\n",
       "      <td>0.020963</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>6812049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.997167</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>6812050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.993865</td>\n",
       "      <td>0.005432</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>6812051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.983942</td>\n",
       "      <td>0.010875</td>\n",
       "      <td>0.005183</td>\n",
       "      <td>6812052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.996165</td>\n",
       "      <td>0.003363</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>6812063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.982246</td>\n",
       "      <td>0.009533</td>\n",
       "      <td>0.008221</td>\n",
       "      <td>6812077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.996720</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>6812082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.980694</td>\n",
       "      <td>0.018120</td>\n",
       "      <td>0.001186</td>\n",
       "      <td>6812091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.979171</td>\n",
       "      <td>0.018506</td>\n",
       "      <td>0.002323</td>\n",
       "      <td>6812098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.977584</td>\n",
       "      <td>0.014818</td>\n",
       "      <td>0.007597</td>\n",
       "      <td>6812106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.905085</td>\n",
       "      <td>0.084500</td>\n",
       "      <td>0.010415</td>\n",
       "      <td>6812119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74629</th>\n",
       "      <td>0.986672</td>\n",
       "      <td>0.011242</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>7699404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74630</th>\n",
       "      <td>0.962754</td>\n",
       "      <td>0.028435</td>\n",
       "      <td>0.008811</td>\n",
       "      <td>7699413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74631</th>\n",
       "      <td>0.959048</td>\n",
       "      <td>0.031538</td>\n",
       "      <td>0.009413</td>\n",
       "      <td>7699418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74632</th>\n",
       "      <td>0.953547</td>\n",
       "      <td>0.031226</td>\n",
       "      <td>0.015227</td>\n",
       "      <td>7699419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74633</th>\n",
       "      <td>0.960412</td>\n",
       "      <td>0.030708</td>\n",
       "      <td>0.008880</td>\n",
       "      <td>7699421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74634</th>\n",
       "      <td>0.954147</td>\n",
       "      <td>0.041561</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>7707462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74635</th>\n",
       "      <td>0.902474</td>\n",
       "      <td>0.088453</td>\n",
       "      <td>0.009073</td>\n",
       "      <td>7707510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74636</th>\n",
       "      <td>0.948011</td>\n",
       "      <td>0.040803</td>\n",
       "      <td>0.011186</td>\n",
       "      <td>7707515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74637</th>\n",
       "      <td>0.965035</td>\n",
       "      <td>0.030976</td>\n",
       "      <td>0.003989</td>\n",
       "      <td>7707520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74638</th>\n",
       "      <td>0.975380</td>\n",
       "      <td>0.020693</td>\n",
       "      <td>0.003927</td>\n",
       "      <td>7707671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74639</th>\n",
       "      <td>0.991382</td>\n",
       "      <td>0.007227</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>7714404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74640</th>\n",
       "      <td>0.970471</td>\n",
       "      <td>0.026579</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>7714406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74641</th>\n",
       "      <td>0.995432</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>7714408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74642</th>\n",
       "      <td>0.657906</td>\n",
       "      <td>0.216156</td>\n",
       "      <td>0.125937</td>\n",
       "      <td>7724353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74643</th>\n",
       "      <td>0.283438</td>\n",
       "      <td>0.394739</td>\n",
       "      <td>0.321823</td>\n",
       "      <td>7724798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74644</th>\n",
       "      <td>0.902068</td>\n",
       "      <td>0.092356</td>\n",
       "      <td>0.005576</td>\n",
       "      <td>7731330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74645</th>\n",
       "      <td>0.984552</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>7742634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74646</th>\n",
       "      <td>0.934971</td>\n",
       "      <td>0.063856</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>7742636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74647</th>\n",
       "      <td>0.967728</td>\n",
       "      <td>0.030577</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>7742642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74648</th>\n",
       "      <td>0.987368</td>\n",
       "      <td>0.011926</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>7742644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74649</th>\n",
       "      <td>0.990090</td>\n",
       "      <td>0.008743</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>7742794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74650</th>\n",
       "      <td>0.600194</td>\n",
       "      <td>0.111467</td>\n",
       "      <td>0.288339</td>\n",
       "      <td>7742859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74651</th>\n",
       "      <td>0.990815</td>\n",
       "      <td>0.007322</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>7742861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74652</th>\n",
       "      <td>0.772578</td>\n",
       "      <td>0.217611</td>\n",
       "      <td>0.009811</td>\n",
       "      <td>7748247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74653</th>\n",
       "      <td>0.798896</td>\n",
       "      <td>0.184709</td>\n",
       "      <td>0.016395</td>\n",
       "      <td>7748250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74654</th>\n",
       "      <td>0.887208</td>\n",
       "      <td>0.092696</td>\n",
       "      <td>0.020096</td>\n",
       "      <td>7748251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74655</th>\n",
       "      <td>0.533297</td>\n",
       "      <td>0.373187</td>\n",
       "      <td>0.093515</td>\n",
       "      <td>7748271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74656</th>\n",
       "      <td>0.201741</td>\n",
       "      <td>0.397385</td>\n",
       "      <td>0.400874</td>\n",
       "      <td>7748273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74657</th>\n",
       "      <td>0.815460</td>\n",
       "      <td>0.093311</td>\n",
       "      <td>0.091229</td>\n",
       "      <td>7754429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74658</th>\n",
       "      <td>0.788890</td>\n",
       "      <td>0.190738</td>\n",
       "      <td>0.020372</td>\n",
       "      <td>7761779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74659 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2  listing_id\n",
       "0      0.065015  0.203845  0.731141     6811958\n",
       "1      0.312899  0.398462  0.288639     6811960\n",
       "2      0.122461  0.357515  0.520024     6811964\n",
       "3      0.957467  0.035353  0.007180     6811971\n",
       "4      0.733189  0.111023  0.155787     6811974\n",
       "5      0.951445  0.030730  0.017825     6811983\n",
       "6      0.410698  0.431817  0.157485     6811984\n",
       "7      0.933338  0.030411  0.036251     6811985\n",
       "8      0.661238  0.146402  0.192360     6811988\n",
       "9      0.984740  0.011803  0.003457     6811990\n",
       "10     0.855623  0.087344  0.057032     6811992\n",
       "11     0.981936  0.012301  0.005763     6811995\n",
       "12     0.995360  0.004043  0.000597     6811997\n",
       "13     0.995629  0.003802  0.000570     6812012\n",
       "14     0.997648  0.001928  0.000424     6812016\n",
       "15     0.933809  0.050804  0.015388     6812032\n",
       "16     0.974869  0.022679  0.002451     6812035\n",
       "17     0.979312  0.013545  0.007143     6812041\n",
       "18     0.953315  0.044381  0.002304     6812045\n",
       "19     0.973253  0.020963  0.005784     6812049\n",
       "20     0.997167  0.001485  0.001349     6812050\n",
       "21     0.993865  0.005432  0.000703     6812051\n",
       "22     0.983942  0.010875  0.005183     6812052\n",
       "23     0.996165  0.003363  0.000473     6812063\n",
       "24     0.982246  0.009533  0.008221     6812077\n",
       "25     0.996720  0.002377  0.000903     6812082\n",
       "26     0.980694  0.018120  0.001186     6812091\n",
       "27     0.979171  0.018506  0.002323     6812098\n",
       "28     0.977584  0.014818  0.007597     6812106\n",
       "29     0.905085  0.084500  0.010415     6812119\n",
       "...         ...       ...       ...         ...\n",
       "74629  0.986672  0.011242  0.002086     7699404\n",
       "74630  0.962754  0.028435  0.008811     7699413\n",
       "74631  0.959048  0.031538  0.009413     7699418\n",
       "74632  0.953547  0.031226  0.015227     7699419\n",
       "74633  0.960412  0.030708  0.008880     7699421\n",
       "74634  0.954147  0.041561  0.004292     7707462\n",
       "74635  0.902474  0.088453  0.009073     7707510\n",
       "74636  0.948011  0.040803  0.011186     7707515\n",
       "74637  0.965035  0.030976  0.003989     7707520\n",
       "74638  0.975380  0.020693  0.003927     7707671\n",
       "74639  0.991382  0.007227  0.001392     7714404\n",
       "74640  0.970471  0.026579  0.002950     7714406\n",
       "74641  0.995432  0.003892  0.000676     7714408\n",
       "74642  0.657906  0.216156  0.125937     7724353\n",
       "74643  0.283438  0.394739  0.321823     7724798\n",
       "74644  0.902068  0.092356  0.005576     7731330\n",
       "74645  0.984552  0.014648  0.000800     7742634\n",
       "74646  0.934971  0.063856  0.001173     7742636\n",
       "74647  0.967728  0.030577  0.001695     7742642\n",
       "74648  0.987368  0.011926  0.000706     7742644\n",
       "74649  0.990090  0.008743  0.001168     7742794\n",
       "74650  0.600194  0.111467  0.288339     7742859\n",
       "74651  0.990815  0.007322  0.001863     7742861\n",
       "74652  0.772578  0.217611  0.009811     7748247\n",
       "74653  0.798896  0.184709  0.016395     7748250\n",
       "74654  0.887208  0.092696  0.020096     7748251\n",
       "74655  0.533297  0.373187  0.093515     7748271\n",
       "74656  0.201741  0.397385  0.400874     7748273\n",
       "74657  0.815460  0.093311  0.091229     7754429\n",
       "74658  0.788890  0.190738  0.020372     7761779\n",
       "\n",
       "[74659 rows x 4 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['high', 'medium', 'low', 'listing_id'], dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.rename(columns={0: 'high', 1: 'medium', 2: 'low'}, inplace=True)\n",
    "result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[['listing_id','high','medium','low']].to_csv('scores_250317.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
